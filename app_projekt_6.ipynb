{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f291a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b5799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/danie/Downloads/SmA-Four-Tank-Batch-Process_V2.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7012645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = []\n",
    "\n",
    "# Entferne redundante Bezeichnungen aus den Spaltennamen\n",
    "for x in df.columns:\n",
    "    col.append(x.split(\" \")[0])\n",
    "    \n",
    "df.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aa5e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entferne alle Schritte mit StepID = 2, da diese für das Projekt nicht relevant sind\n",
    "df.drop(df[df[\"CuStepNo\"] == 2].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f42ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extrahiere die Zeitangaben bzw. formatiere die Zeitstempel in das richtige Format\n",
    "df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "\n",
    "df[\"day\"] = df.timestamp.dt.day\n",
    "df[\"hour\"] = df.timestamp.dt.hour\n",
    "df[\"second\"] = df.timestamp.dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60348d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keine NaN-Werte vorhanden\n",
    "df[df.any(axis = 1).isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfd42f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# determine start and end of steps\n",
    "df['dstep_p']=df['CuStepNo'].diff()\n",
    "df['dstep_n']=df['CuStepNo'].diff(-1)\n",
    "\n",
    "vsteps = [1,7,8,3]\n",
    "\n",
    "# select rows with a step change\n",
    "dfsen=df[(df['dstep_n']!=0)]\n",
    "dfsep=df[(df['dstep_p']!=0)]\n",
    "dfse=pd.concat([dfsen,dfsep])\n",
    "dfse=dfse.sort_values(by=['timestamp'])\n",
    "\n",
    "# create new dataframe where we store extracted information\n",
    "dfinfo_steps=pd.DataFrame(columns=['step_length','start','end','stepn'])\n",
    "\n",
    "# iterative approach\n",
    "pstep=-1\n",
    "c=0\n",
    "for n in range(dfse.shape[0]):\n",
    "    # get row\n",
    "    r=dfse.iloc[n]\n",
    "    if pstep==r['CuStepNo']:\n",
    "        # determine step length\n",
    "        stepl=r['timestamp']-dfse.iloc[n-1]['timestamp']\n",
    "        # update dataframe\n",
    "        dfinfo_steps.loc[c]=(stepl,dfse.iloc[n-1]['timestamp'],r['timestamp'],r['CuStepNo'])\n",
    "        c=c+1\n",
    "    else:\n",
    "        pstep=r['CuStepNo']\n",
    "print(dfinfo_steps.head())\n",
    "print('Max step_length: {}'.format(dfinfo_steps['step_length'].max()))\n",
    "print('Min step_length: {}'.format(dfinfo_steps['step_length'].min()))\n",
    "print('#steps: {}'.format(dfinfo_steps.shape[0]))\n",
    "\n",
    "# now determine whether the batch is complete\n",
    "batchn=1\n",
    "batchi=-1\n",
    "dfinfo_steps[\"batchn\"]=0\n",
    "dfinfo_steps[\"is_complete\"]=False\n",
    "dfinfo_batches=pd.DataFrame(columns=['batch_length','start','end','steps','batchn','is_complete'])\n",
    "n=0\n",
    "b=0\n",
    "while True:\n",
    "    if n+len(vsteps)>dfinfo_steps.shape[0]:\n",
    "        # complete info at incomplete, last batch\n",
    "        steps=[]\n",
    "        for v in range(dfinfo_steps.shape[0]-n):\n",
    "            dfinfo_steps.at[n+v,'batchn']=batchi\n",
    "            dfinfo_steps.at[n+v,'is_complete']=False\n",
    "            steps.append(dfinfo_steps.at[n+v,'stepn'])\n",
    "        dfinfo_batches.loc[b]=[dfinfo_steps.at[n+v,'end']-dfinfo_steps.at[n,'start'],dfinfo_steps.at[n,'start'], \\\n",
    "                               dfinfo_steps.at[n+v,'end'],steps,batchi,False]\n",
    "        b=b+1\n",
    "        break\n",
    "    # check if all steps of a batch are present and in correct order\n",
    "    isCorrect=True\n",
    "    for v in range(len(vsteps)):\n",
    "        isCorrect=dfinfo_steps.loc[n+v,'stepn']==vsteps[v]\n",
    "        if not isCorrect:\n",
    "            break\n",
    "    if isCorrect:\n",
    "        steps=[]\n",
    "        for v in range(len(vsteps)):\n",
    "            dfinfo_steps.at[n+v,'batchn']=batchn\n",
    "            dfinfo_steps.at[n+v,'is_complete']=True\n",
    "            steps.append(dfinfo_steps.at[n+v,'stepn'])\n",
    "        dfinfo_batches.loc[b]=[dfinfo_steps.at[n+v,'end']-dfinfo_steps.at[n,'start'],dfinfo_steps.at[n,'start'], \\\n",
    "                               dfinfo_steps.at[n+v,'end'],steps,batchn,True]\n",
    "        n=n+len(vsteps)\n",
    "        batchn=batchn+1\n",
    "        b=b+1\n",
    "    else:\n",
    "        steps=[]\n",
    "        for vc in range(v):\n",
    "            dfinfo_steps.at[n+vc,'batchn']=batchi\n",
    "            dfinfo_steps.at[n+vc,'is_complete']=False\n",
    "            steps.append(dfinfo_steps.at[n+v,'stepn'])\n",
    "        dfinfo_batches.loc[b]=[dfinfo_steps.at[n+vc,'end']-dfinfo_steps.at[n,'start'],dfinfo_steps.at[n,'start'], \\\n",
    "                               dfinfo_steps.at[n+vc,'end'],steps,batchi,False]\n",
    "        batchi=batchi-1\n",
    "        n=n+vc\n",
    "        b=b+1\n",
    "# save dfinfo_steps to file\n",
    "dfinfo_steps.to_pickle('SmA-Four-Tank-Info-Steps.pkl')\n",
    "dfinfo_batches.to_pickle('SmA-Four-Tank-Info-Batches.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af641ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_steps = pd.read_pickle(\"C:/Users/danie/SmA-Four-Tank-Info-Steps.pkl\")\n",
    "df_batches = pd.read_pickle(\"C:/Users/danie/SmA-Four-Tank-Info-Batches.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b7697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entferne den letzten unvollständigen Batch aus dem Datensatz\n",
    "\n",
    "df = df.drop(df[df.timestamp >= '2018-10-31 14:29:32'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba6c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.DeviationID == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe883f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Entferne einen Zeileneintrag, bei dem die DeviationID 0 ist\n",
    "\n",
    "df = df.drop(df[df.DeviationID == 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f14a5",
   "metadata": {},
   "source": [
    "# DevId\n",
    "### 1 --> Pl1200 (zwischen PL12002 und PL12003)\n",
    "### 2 --> PL1100 (zwischen YC10001 und YC14001)\n",
    "### 3 --> YC21006\n",
    "### 4 --> YC22006\n",
    "### 5 --> YC23006\n",
    "### 6 --> YS14005\n",
    "### 7 --> Pl2150\n",
    "### 8 --> YS14004\n",
    "### 9 --> YS10004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4e02d",
   "metadata": {},
   "source": [
    "Schritt 7 generell am kürzesten, Schritt 8 am längsten (evtl. wegen Pausen zwischen den Batches? An Wochenenden wurden jedoch keine Daten gespeichert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeff36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"CuStepNo\").agg({\"second\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e39a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_steps.groupby(\"stepn\").agg({\"step_length\":\"sum\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1f8c1",
   "metadata": {},
   "source": [
    "#### Wochenenden am:\n",
    "13.-14.\n",
    "\n",
    "20.-21.\n",
    "\n",
    "27.-28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9032c5b",
   "metadata": {},
   "source": [
    "### 16. und 23. Oktober komplett fehlerfrei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f0a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.DeviationID > 1].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13010944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_steps.sort_values(\"step_length\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00597d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Füge Batchnummern und Schrittlängen zum Dataframe hinzu\n",
    "a = df.merge(df_steps, how = \"left\", left_on = \"timestamp\", right_on = \"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efe245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[[\"stepn\"]] = a[[\"stepn\"]].fillna(method = \"ffill\")\n",
    "a[[\"batchn\"]] = a[[\"batchn\"]].fillna(method = \"ffill\")\n",
    "a[\"step_length\"] = a[\"step_length\"].dt.total_seconds()\n",
    "a[[\"step_length\"]] = a[[\"step_length\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dded8b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# redundante oder unnötige Spalten entfernen\n",
    "a.drop(columns = [\"start\", \"end\", \"is_complete\", \"stepn\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b12e9",
   "metadata": {},
   "source": [
    "## Ein Batchprozess benötigt hier zwischen 468 und 1997 Sekunden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e81d48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.groupby([\"batchn\"]).agg({\"timestamp\":\"count\", \"DeviationID\":\"unique\", \"CuStepNo\":\"unique\"}).sort_values(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211ad99",
   "metadata": {},
   "source": [
    "### Schritt 8 in Batch 9 äußerst kurz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd0cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[a.batchn == 9].drop_duplicates(subset = [\"step_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e43451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.groupby(\"DeviationID\").agg({\"batchn\":\"nunique\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a5e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.groupby(\"day\").agg({\"batchn\":\"nunique\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e39ac",
   "metadata": {},
   "source": [
    "### Bei 9 Spalten sind die gemessenene Werte konstant\n",
    "\n",
    "--> nicht nützlich zur Anomaliedetektion/ Klassifizierung, kann entfernt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766be66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "useless_columns = []\n",
    "\n",
    "for x in a.columns:\n",
    "    if a[x].describe()[2] == 0.0:\n",
    "        print(x)\n",
    "        useless_columns.append(x)\n",
    "        print(f\"unique values: {a[x].unique()}\")\n",
    "        \n",
    "a.drop(columns = useless_columns, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb097ba0",
   "metadata": {},
   "source": [
    "Die Werte in PIC14007_SP weisen keine signifikant großen Veränderungen auf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f8ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.drop([\"PIC14007_SP\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59aab5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[\"DeviationID\"] = a.DeviationID.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d343f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstelle neue Spalte timestamp_difference, die den Zeitunterschied seit dem vorherigen Datenpunkt in Sekunden anzeigt.\n",
    "\n",
    "a[\"timestamp_difference\"] = a.timestamp.diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e12ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setze den Wert der zuvor erstellten Spalte bei jedem neuen Batch zu Beginn auf 0\n",
    "\n",
    "a.loc[a.timestamp_difference > 80, \"timestamp_difference\"] = 0\n",
    "a[\"timestamp_difference\"].iloc[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891860c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstelle neue Spalte batch_duration, die pro Batch die gesamt verstrichene Zeit seit Batchbeginn anzeigt.\n",
    "\n",
    "\n",
    "a[\"batch_duration\"] = 0\n",
    "\n",
    "for x in range(1, a.batchn.nunique() + 1):\n",
    "    # Pro Batchnummer wird die Batchzeit berechnet (sonst bekommt man am Ende die Gesamtdauer aller Batches)\n",
    "    a.loc[a.batchn == x, \"batch_duration\"] = a.loc[a.batchn == x, :].timestamp_difference.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9904d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1, a.batchn.nunique() + 1):\n",
    "    for i in a[a.batchn == x].CuStepNo.unique():\n",
    "        a.loc[(a.batchn == x) & (a.CuStepNo == i), \"step_length\"] = a.loc[(a.batchn == x) & (a.CuStepNo == i), :].timestamp_difference.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69a75b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs = []\n",
    "\n",
    "# Erstelle für jede unterschiedliche DeviationID ein Plot, das den Verlauf eines bestimmten Werts pro Batch anzeigt\n",
    "\n",
    "def create_batch_comparisons(data, column):\n",
    "    n = 0\n",
    "    for x in sorted(data.DeviationID.unique()):\n",
    "        fig = px.line(data[(a.batchn == n) & (data.DeviationID == x)],  x = \"batch_duration\", \n",
    "                      y = column, \n",
    "                      title = f\"DeviationID = {x}\"\n",
    "                 )\n",
    "\n",
    "        for n in range(batchn): \n",
    "            fig.add_scatter(x = data[(data.batchn == n) & (data.DeviationID == x)][\"batch_duration\"], \n",
    "                            y = data[(data.batchn == n) & (data.DeviationID == x)][column])\n",
    "        figs.append(fig)\n",
    "        # jedes Element in figs steht für ein Plot mit allen Batches, die die gleiche DeviationID aufweisen\n",
    "        # Achtung! Die traces in der Farblegende sind: batchn des Batches + 1\n",
    "    return figs\n",
    "\n",
    "gg = create_batch_comparisons(a, \"YC14001_MV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05375aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a8c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(0, len(gg)):\n",
    "    gg[e].show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6db40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig_s = []\n",
    "\n",
    "# Erstelle für jede unterschiedliche DeviationID ein Plot, das den Verlauf eines bestimmten Werts pro Batch anzeigt\n",
    "\n",
    "def create_step_comparisons(data, column, CuStep):\n",
    "    n = 0\n",
    "    \n",
    "    \n",
    "    for x in sorted(a.DeviationID.unique()):\n",
    "        fig = px.line(data[(data.batchn == n) & (data.DeviationID == x)],  x = \"step_length\", \n",
    "                      y = column, \n",
    "                      title = f\"DeviationID = {x}, StepID: {CuStep}\", hover_name = \"batchn\",\n",
    "                 )\n",
    "        for n in range(1, batchn + 1):\n",
    "            fig.add_scatter(x = data[(data.batchn == n) & (data.DeviationID == x) & (data.CuStepNo == CuStep)][\"step_length\"], \n",
    "                            y = data[(data.batchn == n) & (data.DeviationID == x) & (data.CuStepNo == CuStep)][column],\n",
    "                            )\n",
    "        fig_s.append(fig)\n",
    "        # jedes Element in figs steht für ein Plot mit allen Batches, die die gleiche DeviationID aufweisen\n",
    "        # Achtung! Die traces in der Farblegende sind: batchn des Batches + 1\n",
    "    return fig_s\n",
    "\n",
    "gg_s = create_step_comparisons(a, \"LevelMainTank\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e8256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(0, len(gg_s)):\n",
    "    gg_s[e].show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff848b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_df[(dtw_df.batchn == 1) & (dtw_df.DeviationID == 1) & (dtw_df.CuStepNo == 1) & (dtw_df.step_length > 25)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af110cb6",
   "metadata": {},
   "source": [
    "## Überblick über alle gemessenen Werte\n",
    "\n",
    "manche Fehlertypen evtl. nur durch Analysieren der zeitlichen Abstände erkennbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ba677",
   "metadata": {},
   "source": [
    "##### LevelMainTank um die Hälfte geringer ab 24.10.?\n",
    "\n",
    "##### PI12002_PV_Out ungefähr am 16.10. auf 0 ?\n",
    "\n",
    "##### FIC14002_SP kurz 0 am 26./27.10. --> Neustart des Systems?\n",
    "\n",
    "### DevID = 2.0 oder 3.0 \n",
    "--> YC14001_MV, FIC14002_MV nur auf 100?\n",
    "\n",
    "### DevID = 4.0 \n",
    "--> FIC14002_PV_Out über 5000, \n",
    "\n",
    "FIC14002_MV unter 30, \n",
    "\n",
    "YC14006_MV sehr hoch, \n",
    "\n",
    "PI14012_PV_Out sehr hoch (auch bei DevID = 3.0)\n",
    "\n",
    "PIC14007_MV sehr hoch\n",
    "\n",
    "\n",
    "### DevID = 8.0 \n",
    "--> YC14006_MV deutlich niedriger, \n",
    "\n",
    "PI14012_PV_Out unter 0 (beide Bauteile hängen miteinander zusammen), \n",
    "\n",
    "YC23001_MV auf 100, \n",
    "\n",
    "FIC23002_MV auf 100, \n",
    "\n",
    "FIC14002_SP minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8f1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.histogram(a.resample(\"5S\", on = \"timestamp\").mean().reset_index(), x = \"batchn\", nbins = 300,\n",
    "            title = \"Wurden alle Batches gleich oft im Datensatz gespeichert?\", color = \"DeviationID\", width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c568ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days = [a[a[\"PI12002_PV_Out\"] < 0].day.unique()[0]]\n",
    "\n",
    "fig_days = px.line(a[(a.day.isin(days)) & (a.timestamp.dt.hour < 18)], x = \"timestamp\", y = \"PI12002_PV_Out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d76e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in a[(a.day.isin(days)) & (a.timestamp.dt.hour < 15)].drop_duplicates([\"batchn\", \"CuStepNo\"]).iloc:\n",
    "    fig_days.add_vline(x = r.timestamp)\n",
    "    fig_days.add_annotation(x = r.timestamp, text = str(r.CuStepNo))\n",
    "fig_days.update_layout(yaxis_range = [-0.2, 0.1])\n",
    "\n",
    "fig_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f0f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for x in a.iloc[:, 3:32].columns:\n",
    "    plt.figure(figsize=(15, 5), dpi=80)\n",
    "    plt.plot(a[\"timestamp\"], a[x])\n",
    "    plt.title(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0dff1",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30751ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dtw = a.drop([\"timestamp\", \"day\", \"hour\", \"second\", \"dstep_p\", \"dstep_n\",\n",
    "               \"timestamp_difference\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e4709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dtw import dtw\n",
    "from tslearn.metrics import dtw_path, dtw\n",
    "\n",
    "# dtw_df soll die neuen Zeitreihen beinhalten\n",
    "dtw_df = pd.DataFrame(columns=a_dtw.columns)\n",
    "\n",
    "dtw_series_list = []\n",
    "\n",
    "# Gehe über jedes Feature\n",
    "for col in a_dtw.columns:\n",
    "    print(col)\n",
    "\n",
    "    dtw_series = []\n",
    "\n",
    "    for x in a_dtw.CuStepNo.unique():\n",
    "        # Datensatz mit nur diesem Schritt\n",
    "        step_df = a_dtw[a_dtw.CuStepNo == x]\n",
    "\n",
    "        # Längste Schrittdauer ermitteln (Achtung, sehr ausreißerempfindlich)\n",
    "        # evtl. mean + 2 * std statt max?\n",
    "       # max_length = int(a_dtw[a_dtw.CuStepNo == x].step_length.max())\n",
    "        max_length = int(a_dtw[a_dtw.CuStepNo == x].step_length.mean() + 1.96 * a_dtw[a_dtw.CuStepNo == x].step_length.std())\n",
    "        \n",
    "        for b in a_dtw.batchn.unique():\n",
    "            \n",
    "            time_series = step_df[step_df.batchn == b][col].to_numpy()\n",
    "\n",
    "            # DTW, um alte Zeitreihe an die neue Größe anzupassen (Berechnen der Distanzmatrizen)\n",
    "            path, dist = dtw_path(time_series, np.zeros(max_length), \n",
    "                                  global_constraint=\"itakura\", \n",
    "                                  itakura_max_slope=10)\n",
    "\n",
    "            # Neue Zeitreihe, die dann in dtw_df übernommen wird\n",
    "            new_time_series = np.zeros(max_length)\n",
    "\n",
    "            # Ersetze die Nullen durch die Werte aus den alten Zeitreihen (beide haben die gleiche Länge)\n",
    "            for i, j in path:\n",
    "                new_time_series[j] = time_series[i]\n",
    "\n",
    "            # Werte an dtw_series ranhängen\n",
    "            dtw_series.append(new_time_series)\n",
    "    \n",
    "    # enthält alle bisher aufgenommenen Daten, könnte für einen effizienteren Algorithmus angepasst werden\n",
    "    dtw_series_list.append(dtw_series)\n",
    "\n",
    "# Für jedes Feature und jeden Schritt werden die Werte aus dtw_series zu dtw_df hinzugefügt, and add them to the new DataFrame\n",
    "for i, col in enumerate(a_dtw.columns):\n",
    "    dtw_df[col] = pd.concat([pd.Series(batch) for batch in dtw_series_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitunterschied zwischen den interpolierten Zeilen\n",
    "\n",
    "dtw_df[\"timestamp_difference\"] = 1\n",
    "\n",
    "# Erstelle die Werte für step_length wie zuvor beim DataFrame a\n",
    "\n",
    "for x in range(1, dtw_df.batchn.nunique() + 1):\n",
    "    for i in dtw_df[dtw_df.batchn == x].CuStepNo.unique():\n",
    "        dtw_df.loc[(dtw_df.batchn == x) & (dtw_df.CuStepNo == i), \"step_length\"] = dtw_df.loc[(dtw_df.batchn == x) & (dtw_df.CuStepNo == i), :].timestamp_difference.cumsum()\n",
    "        \n",
    "# Spalte kann wieder entfernt werden, da nicht mehr benötigt\n",
    "dtw_df.drop(\"timestamp_difference\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(a[(a.DeviationID == 1) & (a.CuStepNo == 1)],  x = \"step_length\", \n",
    "                      y = \"LevelMainTank\", color = \"batchn\",\n",
    "                      title = f\"Datensatz vor DTW - DeviationID = {1}, CuStepNo: {1}\",\n",
    "                 ).update_layout(xaxis_range = [0, 323])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(dtw_df[(dtw_df.DeviationID == 1) & (dtw_df.CuStepNo == 1)],  x = \"step_length\", \n",
    "                      y = \"LevelMainTank\", color = \"batchn\",\n",
    "                      title = f\"Datensatz nach DTW - DeviationID = {1}, CuStepNo: {1}\",\n",
    "                 ).update_layout(xaxis_range = [0, 323])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(a[(a.batchn == 23) & (a.DeviationID == 1) & (a.CuStepNo == 1)],  x = \"step_length\", \n",
    "                      y = \"LevelMainTank\", \n",
    "                      title = f\"Originaler Datensatz - DeviationID = {1}, batchn: {23}, CuStepNo: {1}\"\n",
    "                 ).update_layout(xaxis_range = [0, 323])                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(dtw_df[(dtw_df.batchn == 23) & (dtw_df.DeviationID == 1) & (dtw_df.CuStepNo == 1)],  x = \"step_length\", \n",
    "                      y = \"LevelMainTank\", \n",
    "                      title = f\"Datensatz nach DTW - DeviationID = {1}, batchn: {22}, CuStepNo: {1}\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959f63f",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd3f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b85da0",
   "metadata": {},
   "source": [
    "Besser auch batchn entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_df.drop(\"batch_duration\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b594b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wir erstellen Trainings- und Testdaten in einem Verhältnis von 70:30. \n",
    "# Da Batches mit verschiedenen DevIDs chronologisch durchgegangen werden (bspw. kommen Batches mit DevID == 8 und == 10\n",
    "# vermehrt in den letzten Tagen des erfassten Zeitraums), sammeln wir pro DeviationID 70% der Batches. \n",
    "# D.h. aus Batches mit DevID == 1 werden 70% dem Trainingsdatensatz hinzugefügt, die restlichen 30% dem Testdatensatz usw.\n",
    "\n",
    "# train_test_split würde zu einer willkürlichen Aufteilung führen, die das Prinzip der Unabhängigkeit zwischen den\n",
    "# Datenpunkten verletzen würden. Wir wollen die einzelnen Batches aufteilen.\n",
    "\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for dev_id in dtw_df.DeviationID.unique():\n",
    "    #print(dev_id)\n",
    "    batches_with_devid = dtw_df[dtw_df.DeviationID == dev_id].batchn.unique()\n",
    "    num_batches = len(batches_with_devid)\n",
    "    train_batches = int(round(num_batches * 0.7))\n",
    "    last_index_train = batches_with_devid[train_batches - 1]\n",
    "    \n",
    "    for b in batches_with_devid:\n",
    "        if b <= last_index_train:\n",
    "            train = train.append(dtw_df[(dtw_df.DeviationID == dev_id) & (dtw_df.batchn == b)])\n",
    "        else:\n",
    "            test = test.append(dtw_df[(dtw_df.DeviationID == dev_id) & (dtw_df.batchn == b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354985c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Speichere die Trainings- und Testdaten als Backup\n",
    "\n",
    "backup_train = train.copy()\n",
    "backup_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ab274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train.pop(\"DeviationID\")\n",
    "y_test = test.pop(\"DeviationID\")\n",
    "\n",
    "X_train = train\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51a959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ist kommentiert, da das Modell zu lange läuft (zuletzt am 03.04., vor DTW, ausprobiert)\n",
    "\n",
    "\"\"\"#custom metric\n",
    "def DTW(a, b):   \n",
    "    an = a.size\n",
    "    bn = b.size\n",
    "    pointwise_distance = distance.cdist(a.reshape(-1,1),b.reshape(-1,1))\n",
    "    cumdist = np.matrix(np.ones((an+1,bn+1)) * np.inf)\n",
    "    cumdist[0,0] = 0\n",
    "\n",
    "    for ai in range(an):\n",
    "        for bi in range(bn):\n",
    "            minimum_cost = np.min([cumdist[ai, bi+1],\n",
    "                                   cumdist[ai+1, bi],\n",
    "                                   cumdist[ai, bi]])\n",
    "            cumdist[ai+1, bi+1] = pointwise_distance[ai,bi] + minimum_cost\n",
    "\n",
    "    return cumdist[an, bn]\n",
    "\n",
    "#train\n",
    "parameters = {'n_neighbors':[10]}\n",
    "clf = GridSearchCV(KNeighborsClassifier(metric=DTW), parameters, cv=2, verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21a05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trainiere einen Autoencoder auf skalierten Daten\n",
    "# Über eine Encoderschicht mit ReLu-Aktivierungsfunktion und mit 2 Neuronen sollen die Daten rekonstruiert werden\n",
    "# Weichen die rekonstruierten Daten zu stark von den originalen Inputdaten ab, meldet der Autoencoder eine Anomalie\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import torch\n",
    "\n",
    "X_train_tensor =  np.asarray(X_train).astype('float32')\n",
    "X_test_tensor =  np.asarray(X_test).astype('float32')\n",
    "y_train_tensor =  np.asarray(y_train).astype('float32')\n",
    "y_test_tensor =  np.asarray(y_test).astype('float32')\n",
    "\n",
    "\n",
    "input_layer = Input(shape = (X_train.shape[1], ))\n",
    "encoder_layer = Dense(10, activation = \"relu\")(input_layer)\n",
    "decoder_layer = Dense(X_train.shape[1], activation = \"linear\")(encoder_layer)\n",
    "        \n",
    "autoencoder = Model(inputs = input_layer, outputs = decoder_layer)\n",
    "        \n",
    "\n",
    "        \n",
    "autoencoder.compile(optimizer = SGD(learning_rate=0.01), loss = \"mse\")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "autoencoder.fit(X_train_tensor, y_train_tensor,\n",
    "                epochs=5,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test_tensor, y_test_tensor),\n",
    "                callbacks=[early_stopping])\n",
    "\n",
    "# use autoencoder to detect anomalies\n",
    "threshold = np.mean(autoencoder.predict(X_train_tensor) - X_train_tensor) + 2 * np.std(autoencoder.predict(X_train_tensor) - X_train_tensor)\n",
    "anomalies = np.where(autoencoder.predict(X_test_tensor) - X_test_tensor > threshold, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(anomalies, axis = 1)\n",
    "print(f\"Im Testdatensatz werden an {len(sums[sums > 0])} Datenpunkten Anomalien gemeldet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Der Testdatensatz besteht aus {y_test.shape[0]} Datenpunkten, wobei {y_test[y_test == 1].shape[0]} Datenpunkte eine DeviationID > 1 aufweisen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87146218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
